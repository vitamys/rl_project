{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import simglucose\n",
            "from simglucose.simulation.scenario import CustomScenario\n",
            "import gymnasium as gym\n",
            "from gymnasium.wrappers import FlattenObservation\n",
            "from collections import namedtuple, deque\n",
            "import numpy as np\n",
            "import warnings\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torch.nn.functional as F\n",
            "import random\n",
            "import matplotlib.pyplot as plt\n",
            "import datetime as dt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Filter out deprecation warnings\n",
            "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "TODO: In testing the paper does a --> B. Step 2: Personalization of DQN-Learning Models on Patient-Specific Data.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Classes and Fuctions"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class ReplayMemory(object):\n",
            "\n",
            "    def __init__(self, capacity):\n",
            "        self.memory = deque([],maxlen=capacity)\n",
            "\n",
            "    def push(self, *args):\n",
            "        \"\"\"Save a transition\"\"\"\n",
            "        self.memory.append(Transition(*args))\n",
            "\n",
            "    def sample(self, batch_size):\n",
            "        return random.sample(self.memory, batch_size)\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.memory)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def paper_reward_function(BG_last_hour):\n",
            "    G = BG_last_hour[-1]\n",
            "    if G >= 70 and G <= 180:\n",
            "        return 0.5\n",
            "    if G > 180 and G <= 200:\n",
            "        return -0.9\n",
            "    if G > 200 and G <= 250:\n",
            "        return -1.2\n",
            "    if G > 250 and G <= 350:\n",
            "        return -1.5\n",
            "    if G > 30 and G < 70:\n",
            "        return -1.8\n",
            "    else:\n",
            "        return -2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_env(name):\n",
            "\n",
            "    env = gym.make(name)\n",
            "\n",
            "    env = FlattenObservation(env)\n",
            "\n",
            "    return env"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class DQNNetwork(nn.Module):\n",
            "    def __init__(self, input_size, output_size):\n",
            "        super(DQNNetwork, self).__init__()\n",
            "        self.fc1 = nn.Linear(input_size, 32)\n",
            "        self.fc2 = nn.Linear(32, 16)\n",
            "        self.fc3 = nn.Linear(16, output_size)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = torch.relu(self.fc1(x))\n",
            "        x = torch.relu(self.fc2(x))\n",
            "        return self.fc3(x)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class DDQNAgent:\n",
            "    def __init__(\n",
            "        self,\n",
            "        input_size,\n",
            "        output_size,\n",
            "        gamma=0.95,\n",
            "        learning_rate=0.001,\n",
            "        buffer_size=800,\n",
            "        batch_size=32,\n",
            "    ):\n",
            "        self.gamma = gamma\n",
            "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "        # Epsilon-greedy exploration\n",
            "        self.epsilon_start = 1.0\n",
            "        self.epsilon_end = 0.1\n",
            "        self.epsilon_decay_episodes = 30\n",
            "\n",
            "        self.epsilon = self.epsilon_start\n",
            "\n",
            "        # Q-networks\n",
            "        self.q_network = DQNNetwork(input_size, output_size).to(self.device)\n",
            "        self.target_q_network = DQNNetwork(input_size, output_size).to(self.device)\n",
            "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
            "\n",
            "        # Optimizer\n",
            "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
            "\n",
            "        # Experience replay buffer\n",
            "        self.buffer_size = buffer_size\n",
            "        self.batch_size = batch_size\n",
            "        self.replay_buffer = deque(maxlen=buffer_size)\n",
            "\n",
            "    def select_action(self, state):\n",
            "        if np.random.rand() < self.epsilon:\n",
            "            action = env.action_space.sample()\n",
            "        else:\n",
            "            # Exploit: choose the action with the highest Q-value\n",
            "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
            "            q_values = self.q_network(state_tensor)\n",
            "            action = q_values.argmax().item()\n",
            "\n",
            "        return action\n",
            "\n",
            "    # Update epsilon during training\n",
            "    def update_epsilon(self, episode):\n",
            "        # Linear decay from epsilon_start to epsilon_end over epsilon_decay_episodes\n",
            "        self.epsilon = max(self.epsilon_end, self.epsilon_start - (episode / self.epsilon_decay_episodes))\n",
            "\n",
            "    def store_transition(self, state, action, reward, next_state, done):\n",
            "        transition = (state, action, reward, next_state, done)\n",
            "        self.replay_buffer.append(transition)\n",
            "\n",
            "    def sample_batch(self):\n",
            "        batch = random.sample(\n",
            "            self.replay_buffer, min(len(self.replay_buffer), self.batch_size)\n",
            "        )\n",
            "        states, actions, rewards, next_states, dones = zip(*batch)\n",
            "\n",
            "        # Convert to tensors and handle dimensions\n",
            "        states = torch.stack([torch.FloatTensor(state) for state in states])\n",
            "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
            "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
            "        next_states = torch.stack(\n",
            "            [torch.FloatTensor(next_state) for next_state in next_states]\n",
            "        )\n",
            "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
            "\n",
            "        return states, actions, rewards, next_states, dones\n",
            "\n",
            "    def update_q_network(self):\n",
            "        if len(self.replay_buffer) < self.batch_size:\n",
            "            return\n",
            "\n",
            "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
            "        # put all to device\n",
            "        states = states.to(self.device)\n",
            "        actions = actions.to(self.device)\n",
            "        rewards = rewards.to(self.device)\n",
            "        next_states = next_states.to(self.device)\n",
            "        dones = dones.to(self.device)\n",
            "\n",
            "        # Compute Q-values\n",
            "        q_values = self.q_network(states).gather(1, actions)\n",
            "\n",
            "        # Compute target Q-values using the target network\n",
            "        target_q_values = (\n",
            "            rewards\n",
            "            + (1 - dones)\n",
            "            * self.gamma\n",
            "            * self.target_q_network(next_states).max(1)[0].detach().unsqueeze(1)\n",
            "        )\n",
            "     \n",
            "        # Compute the Huber loss\n",
            "        loss = F.smooth_l1_loss(q_values, target_q_values)\n",
            "\n",
            "        # Update the Q-network\n",
            "        self.optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        self.optimizer.step()\n",
            "\n",
            "    def update_target_q_network(self):\n",
            "        # Update the target network by copying the Q-network parameters\n",
            "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
            "\n",
            "    def train_step(self, state, action, reward, next_state, done):\n",
            "        # Store the transition in the replay buffer\n",
            "        self.store_transition(state, action, reward, next_state, done)\n",
            "\n",
            "        # Update the Q-network\n",
            "        self.update_q_network()\n",
            "\n",
            "        # Update the target Q-network periodically\n",
            "        if len(self.replay_buffer) % 50 == 0:\n",
            "            self.update_target_q_network()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class FlattenAction(gym.ActionWrapper):\n",
            "    \"\"\"Action wrapper that flattens the action.\"\"\"\n",
            "\n",
            "    def __init__(self, env):\n",
            "        super(FlattenAction, self).__init__(env)\n",
            "        self.action_space = gym.spaces.utils.flatten_space(self.env.action_space)\n",
            "\n",
            "    def action(self, action):\n",
            "        return gym.spaces.utils.unflatten(self.env.action_space, action)\n",
            "\n",
            "    def reverse_action(self, action):\n",
            "        return gym.spaces.utils.flatten(self.env.action_space, action)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def evaluate_policy(policy, env_name, seed, eval_episodes=10, max_timesteps=1000):\n",
            "    eval_env = create_env(env_name)\n",
            "    eval_env.unwrapped.seed(seed + 100)\n",
            "\n",
            "    avg_reward = 0.0\n",
            "\n",
            "    for _ in range(eval_episodes):\n",
            "        state, info = eval_env.reset()\n",
            "        done = False\n",
            "\n",
            "        for _ in range(max_timesteps):\n",
            "            action = policy.select_action(state)\n",
            "            state, reward, done, _, info = eval_env.step(action)\n",
            "\n",
            "            avg_reward += reward\n",
            "\n",
            "            if done:\n",
            "                break\n",
            "\n",
            "    avg_reward /= eval_episodes\n",
            "    return avg_reward\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "Transition = namedtuple('Transition',\n",
            "                        ('state', 'action', 'next_state', 'reward', 'done', 'episode'))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "gym.envs.register(\n",
            "    id=\"simglucose-bolus\",\n",
            "    entry_point=\"simglucose.envs:T1DSimEnvBolus\",\n",
            "    kwargs={\n",
            "        \"patient_name\": [\"adolescent#001\", \n",
            "                         \"adolescent#002\", \n",
            "                         \"adolescent#003\", \n",
            "                         \"adolescent#004\", \n",
            "                         \"adolescent#005\", \n",
            "                         \"adolescent#006\", \n",
            "                         \"adolescent#007\", \n",
            "                         \"adolescent#008\", \n",
            "                         \"adolescent#009\", \n",
            "                         \"adolescent#010\"],\n",
            "        \"reward_fun\": paper_reward_function,\n",
            "        \"history_length\": 1,\n",
            "        \"enable_meal\": True,\n",
            "    },\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# create train env\n",
            "env = create_env('simglucose-bolus')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# DDQ Agent setup\n",
            "state_size = env.observation_space.shape[0]\n",
            "action_size = env.action_space.n\n",
            "print('state_size', state_size)\n",
            "print('action_size', action_size)\n",
            "agent = DDQNAgent(input_size=state_size, output_size=action_size)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "replay_memory_size = 800\n",
            "memory = ReplayMemory(replay_memory_size)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_episodes = 100\n",
            "\n",
            "for episode in range(num_episodes):\n",
            "    state, info = env.reset()\n",
            "    total_reward = 0\n",
            "    done = False\n",
            "\n",
            "    while not done:  # End the episode if the environment signals that it's done\n",
            "        action = agent.select_action(state)\n",
            "\n",
            "        # Take the selected action in the environment\n",
            "        next_state, reward, done, _, info = env.step(action)\n",
            "\n",
            "        # Store the transition and perform a training step\n",
            "        agent.train_step(state, action, reward, next_state, done)\n",
            "\n",
            "        memory.push(state, action, next_state, reward, done, episode + 1)\n",
            "\n",
            "        state = next_state\n",
            "\n",
            "        # Accumulate the total reward\n",
            "        total_reward += reward\n",
            "\n",
            "    # Update epsilon\n",
            "    agent.update_epsilon(episode)\n",
            "\n",
            "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#state\n",
            "#observation [GCM, CHO, Insulin]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# visualize results from the memory\n",
            "plt.figure(figsize=(10, 10))\n",
            "plt.subplot(3, 2, 1)\n",
            "gcm_values = [transition.state[0] for transition in memory.memory]\n",
            "\n",
            "plt.plot(gcm_values)\n",
            "plt.xlabel(\"Episode\")\n",
            "plt.ylabel(\"GCM\")\n",
            "\n",
            "plt.subplot(3, 2, 2)\n",
            "rewards = [transition.reward for transition in memory.memory]\n",
            "\n",
            "plt.plot(rewards)\n",
            "plt.xlabel(\"Episode\")\n",
            "plt.ylabel(\"Total Reward\")\n",
            "\n",
            "plt.subplot(3, 2, 3)\n",
            "insulin_values = [transition.state[2] for transition in memory.memory]\n",
            "\n",
            "plt.plot(insulin_values)\n",
            "plt.xlabel(\"Episode\")\n",
            "plt.ylabel(\"Insulin\")\n",
            "\n",
            "plt.subplot(3, 2, 4)\n",
            "cho_values = [transition.state[1] for transition in memory.memory]\n",
            "\n",
            "plt.plot(cho_values)\n",
            "plt.xlabel(\"Episode\")\n",
            "plt.ylabel(\"CHO\")\n",
            "\n",
            "plt.subplot(3, 2, 5)\n",
            "# visualize the actions taken\n",
            "actions_idx = [transition.action for transition in memory.memory]\n",
            "\n",
            "actions_percentage = np.array([0.75, 0.8, 0.9, 1, 1.1, 1.2, 1.25]) - 1\n",
            "actions = actions_percentage[actions_idx]\n",
            "\n",
            "plt.plot(actions)\n",
            "plt.xlabel(\"Episode\")\n",
            "plt.ylabel(\"Action (%)\")\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "#'state', 'action', 'next_state', 'reward', 'done', 'episode'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "avg_reward =evaluate_policy(agent, \"simglucose-bolus\", 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "avg_reward\t"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Evaluation Metrics: \n",
            "- Section V. A:  we de- rived three metrics related to the the percentage of time spent within the different glycemic ranges, that is the normoglycemic range (TIR), i.e., 70 ≤ CGM ≤ 180 mg/dL, below this range (TBR), i.e., CGM < 70 mg/dL, and above this range (TAR), i.e., CGM > 180 mg/dL. \n",
            "- Figure 4"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "rl_project",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.7"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
