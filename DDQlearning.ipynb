{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from simglucose.envs import T1DSimEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I don't know how to use the variables that the model says or if we even have that information available.\n",
    "- For the moment the loop is only over one patient, not properly training yet\n",
    "- Implement reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Define your neural network architecture\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### select_action funtion\n",
    "\n",
    "1. The chosen action $a_t$ represents a percentage modulation $alpha_t$ of the mealtime insulin dose suggested by the standard therapy.\n",
    "2. The modulation factor $alpha_t$) is randomly chosen from a set of possible percentage modulations.\n",
    "3. The insulin amount suggested by the DDQN algorithm at time step $t$ is calculated using the formula: $BC_{ddqn}(t) = BC_s(t) + \\alpha_t \\cdot BC_s(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DDQ Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, gamma=0.99, learning_rate=0.001):\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-networks\n",
    "        self.q_network = QNetwork(input_size, output_size).to(self.device)\n",
    "        self.target_q_network = QNetwork(input_size, output_size).to(self.device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        action_index = q_values.argmax(1).item()\n",
    "        # Possible percentage modulations of the insulin dose\n",
    "        modulation_percentages = [25, 20, 10, 0, -10, -20, -25]\n",
    "        # now selecting the alpha percentage as a random choice from the modulation percentages\n",
    "        alpha_percentage = np.random.choice(modulation_percentages)\n",
    "        alpha_factor = 1 + (alpha_percentage / 100.0)  # Convert percentage to a factor\n",
    "\n",
    "        # Apply the modulation to the insulin dose suggested by standard therapy (BCs)\n",
    "        action_value = alpha_factor * action_index\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def update_q_network(self, state, action, reward, next_state, done):\n",
    "        state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).to(self.device)\n",
    "        action_tensor = torch.FloatTensor([action]).to(self.device)\n",
    "        reward_tensor = torch.FloatTensor([reward]).to(self.device)\n",
    "\n",
    "        # Q-value prediction for all actions\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        # Action_tensor and q_values shapes must match\n",
    "        action_tensor = action_tensor.expand_as(q_values)\n",
    "\n",
    "        # Q-value for the selected action\n",
    "        selected_q_value = torch.sum(q_values * action_tensor, dim=0, keepdim=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double Q-learning: Use the target network for action selection\n",
    "            next_q_values = self.target_q_network(next_state_tensor)\n",
    "            next_action_index = next_q_values.argmax(0, keepdim=True)\n",
    "            target_q_values = reward_tensor + self.gamma * next_q_values.gather(0, next_action_index) * (1 - done)\n",
    "\n",
    "        # Q-network loss\n",
    "        loss = nn.functional.mse_loss(selected_q_value, target_q_values)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        # Update the target Q-network by copying the parameters from the current Q-network\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simglucose environment setup\n",
    "patient_name = \"adolescent#010\"\n",
    "env = T1DSimEnv(patient_name=patient_name)\n",
    "\n",
    "# DDQ Agent setup\n",
    "state_size = len(env.observation_space.low)  # Size of the state vector\n",
    "action_size = len(env.action_space.low)  # Size of the action space\n",
    "agent = DQNAgent(input_size=state_size, output_size=action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step(observation=Observation(CGM=149.80502445158902), reward=0.21085887551353366, done=False, info={'sample_time': 3.0, 'patient_name': 'adolescent#010', 'meal': 0.0, 'patient_state': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.69891197e+02,\n",
       "        1.65975038e+02,  6.30215227e+00, -1.33136366e-02,  1.08236338e+02,\n",
       "        1.08239971e+02,  3.58486790e+00,  8.84671285e+01,  7.75725653e+01,\n",
       "        2.72581852e+02]), 'time': datetime.datetime(2018, 1, 1, 15, 3), 'bg': 147.92056439066695, 'lbgi': 0.0, 'hbgi': 2.611626127620977, 'risk': 2.611626127620977})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "action = agent.select_action(state)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -97.87264021170513, GCM: 593.3493234822606\n",
      "Episode: 2, Total Reward: -97.30607057598033, GCM: 600.0\n",
      "Episode: 3, Total Reward: -96.29777539036654, GCM: 600.0\n",
      "Episode: 4, Total Reward: -92.08636765128593, GCM: 598.7223567284018\n",
      "Episode: 5, Total Reward: -94.87752929811349, GCM: 597.4718313861013\n",
      "Episode: 6, Total Reward: -95.176758063922, GCM: 595.0771923357318\n",
      "Episode: 7, Total Reward: -95.27126757951615, GCM: 600.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "capi_return is NULL\n",
      "Call-back cb_fcn_in___user__routines failed.\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/simglucose/patient/t1dpatient.py\", line 170, in model\n",
      "Fatal Python error: F2PySwapThreadLocalCallbackPtr: F2PySwapThreadLocalCallbackPtr: PyLong_AsVoidPtr failed\n",
      "Python runtime state: initialized\n",
      "    Kmt = params.Km0\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 6201, in __getattr__\n",
      "    and self._info_axis._can_hold_identifiers_and_holds_name(name)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 5414, in _can_hold_identifiers_and_holds_name\n",
      "    is_object_dtype(self.dtype)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/common.py\", line 165, in is_object_dtype\n",
      "    return _is_dtype_type(arr_or_dtype, classes(np.object_))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8, Total Reward: -95.31375314585537, GCM: 597.2737141061987\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 1000\n",
    "TARGET_UPDATE_FREQUENCY = 10\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    first_iteration = True\n",
    "\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Step through the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update Q-network\n",
    "        agent.update_q_network(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update target Q-network periodically\n",
    "        if episode % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            agent.update_target_q_network()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, GCM: {next_state.CGM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGM Measurement: 161.3944732729466\n",
      "Blood Glucose: 154.13309589916958\n",
      "Current Time: 2018-01-01 11:03:00\n",
      "Meal Time: 0.0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Assuming 'env' is your SimGlucose environment\n",
    "env = T1DSimEnv()\n",
    "\n",
    "# Reset the environment to get the initial state\n",
    "initial_observation = env.reset()\n",
    "\n",
    "# Take a step in the environment\n",
    "action = agent.select_action(initial_observation)\n",
    "next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Access information from the 'info' dictionary\n",
    "cgm_measurement = next_observation.CGM\n",
    "blood_glucose = info['bg']\n",
    "current_time = info['time']\n",
    "meal_time = info['meal']\n",
    "\n",
    "# Print or use the information as needed\n",
    "print(f\"CGM Measurement: {cgm_measurement}\")\n",
    "print(f\"Blood Glucose: {blood_glucose}\")\n",
    "print(f\"Current Time: {current_time}\")\n",
    "print(f\"Meal Time: {meal_time}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
